---
title: “分布式数据库测试”这件小事
date: 2023-05-04 10:23:00 +08:00
permalink: /blogs/10017
tags: [work, database, testing]
categories: [稍微正经点的]
---

4年前，我对分布式、数据库、测试这几个领域都很陌生，好奇心与缘分让我和它们相见。
4年相处的经历就像沙子洒在我的脑平面上，很多已经被覆盖，可见之处应该都值得被铭记吧。

分布式、数据库、测试，每一个都有说不完的故事，而我和它们的故事，则要从测试开始说起。

## QA 就是测试，测试就是 QA？

说到测试，我现在会很本能的想到 QA 这个词。我之前的工作职位叫作 QA，
别的同事也会用这个称呼来代指我的岗位或我所在的小组。“QA 就是测试，测试就是 QA ”，
这可能是很多人的直观印象和认识，我一开始差不多也是这样理解的。

但仔细想想，就会发现这个理解有很多说不通的地方。写单测是测试，研发要写单测，
研发是 QA【纪晓岚式微笑】。但这个说法显然会让少部分研发“恼羞成怒”，
你才是 QA，你全家都是 QA【狗头】。

我以前特意搜过，“什么是 QA，QA 是干啥的？”等类似问题。然后我看了好几篇文章，
当时还做了点[笔记][notes-about-qa]。其中有一篇是“网红”文章，它是左耳朵耗子在 2012
年写的《我们需要专职的QA吗？》。我觉得这个“标题”放在 2023 年，仍然足够吸引眼球。
还有两篇是《测试团队与咖啡店》和《软件测试团队“核心价值”的思考》，它们同样引人思考。
我个人觉得这些文章都反映了一个问题，**对于如何做好测试这件事情，
行业里还没有一个开源的范本**，大家要走的路还很长。一位同事说华为发布出来的产品质量不错。
华为对质量比较看重，有完善的流程支持，质量保障体系比较完善。

QA 全称是质量保证（Quality Assurance），不管是 QA 或是测试这个事情，
它们共同的 *宏伟* 目标是保障产品质量。作为一名 QA，当时我就在想，
怎样衡量产品质量？什么是保障的好，什么是不好。

## 质量好坏的量化

“质量好坏”短期往往是难以量化的，数据库业界至今也没有一个好的标准。
测试覆盖率、产品问题数等指标虽然有一定的参考意义，但也有一些显然的不足。
比如，代码行覆盖率即使很高，质量仍然可能很差，而“分支覆盖率”/“MC/DC覆盖率”
等指标又存在“全集难以计算”或“提升成本很高”等问题。对于一个讲究“快速迭代”的行业，
产品问题数则和用户数、新特性数量、以及用户使用场景/时长有明显相关性，
同比环比等计算策略在这样的情况下同样无法使用。

回到 QA 的话题，假设质量好坏能量化，其能反应 QA 工作的好坏么？显然也不能。
举个例子，某个模块质量变好了，完全可能是因为代码换了一位“牛人”来写，鲁棒性更好了。
既然如此，是时候改变一下 QA 的定位和目标了。

## 行业需要 QA 吗？

前段时间一个群在讨论数据库测试的事情，有ex同事说
> 好的数据库，不需要 QA。
> 倒逼研发在开发环节的开头就控制住质量。
> 丰田模式。

然后又有一个ex同事截图了[《分布式系统测试那些事儿 - 理念》][max-blog-1]这篇文章的一小段话
> 我们现在很有意思的一个事情是，迄今为止 PingCAP 没有一个测试人员，
> 这是在所有的公司看来可能都是觉得不可思议的事情

这其实和左耳朵耗子 2012 年那文章讨论的问题有点像，“我们需要 QA 吗？”。
2023 年了，这问题仍然会被大家拿出来讨论，感觉很有意思。

个人觉得《测试团队与咖啡店》这篇文章的观点不错，
（不错还不足以形容我对它的看法，很好！三年前的我应该是没能领悟这段话的含义。）
> 当开发人员对测试团队说“我需要你们在每次提交后对产品进行测试”，
> 他真正想要的只不过是能够有一种机制，使得每次代码提交之后都能验证产品是不是存在明显的问题。
> 通过 CI、分层的自动化测试，测试团队可以用更轻松、更快捷也更优雅的方式解决开发团队的问题。
> 在这个基础上，也许你还可以说服开发团队建立一系列的标准，用于评估产品的生产率和质量，
> 让测试团队和开发团队一起推动持续的生产率增长和质量提升。

写代码，测试，流程规范与思想理念，每个都得要。谁来做呢？

## 分布式数据库的测试

要保障数据库一个功能或特性的质量，制定合理的测试计划，设计有效的测试用例。
除了要遵守质量保障的流程规范，了解常见的测试用例设计方法，更需要了解甚至熟悉这个功能，
从用户使用方法到底层实现原理。

分布式数据库所涉及的知识面是非常广的，且每个具体的垂直领域都会涉及比较深。
从存储、共识算法、事务、执行器、优化器、网络通信协议等基础理论，到用户负载、
数据库基础运维，再到特定场景或问题的解决方案，要接触了解每一个点，就需要很丰富的积累，
更别说熟悉它们了。可能只有“架构师”的心里才能完整的绘出这副图。

从用户使用角度来看，用户和分布式数据库直接交互的方式主要有三类。第一类是通过 SQL
与之进行数据信息交换。第二类是运维操作，比如扩缩容、滚动重启、配置修改等。
第三类则是数据备份恢复、数据同步等。简单使用数据库的这三类功能，
我相信大部分人稍微花点时间都能掌握个大概。

以 TiDB 的 [GC in Compaction Filter][gc-in-c-f] 特性为例，在有固定流程规范的背景下，
**谁花最少的时间就能学习并掌握这个特性的测试工作呢**？我觉得是写代码的人。
因为其它人学 RocksDB 和 Compaction Filter 等繁琐的原理效率很可能不够高。
不过听一位以前的同事说，华为是让测试团队的人来学习，写代码的人负责讲解。
我认为这样做的代价是周期更长，需要的人非常多，好处则是变相引入了 reviewer，测试会更周全。

以上是把“数据库的一个非常细节的功能”当成一个功能/特性，尝试给出我个人对上面“谁来做呢”
这个问题的理解。但如果变换一个看问题的粒度，把 CAP 中的 C 或者 A
当成分布式数据库的一个特性呢，或者把数据库的性能看成一个特性呢，
我觉得上述问题则可以有完全不同的答案。

### 系统测试

我觉得自己算是一个喜欢折腾概念的人，因为我感觉概念最能让一个东西从模糊变得具体。
因此我工作的时候花了些时间就去寻找系统测试的概念，比如查维基百科、看《软件测试的艺术》这本书，
但我记忆中，它们都没有给出让我非常信服的定义。比如维基百科是这么说的
> 系统测试是将需测试的软件，作为整个基于计算机系统的一个元素，
> 与计算机硬件、外设、某些支持软件、数据和人员等其他系统元素及环境结合在一起测试。
> 在实际运行环境下，对计算机系统进行一系列的组装测试和确认测试。
> 系统测试的目的在于通过与系统的软件需求作比较，发现软件与系统定义不符合或与之矛盾的地方。

我觉得它说的可能没错，未来的某一天，说不定我能理解。但现阶段我觉得它讲的还是过于模糊了。
不过这里有句话还是有点启发的 **系统测试的目的在于通过与系统的软件需求作比较**。
那本书，我记忆中（我的书怎么不见了，尴尬）没有下定义，但它把性能测试、
配置测试等都划分在系统测试这一章节下，作为其子章节。

我也想当一回发明家，给它来个定义如下
> 系统测试是对系统大粒度垂直领域进行的测试，
> 比如性能测试、稳定性测试、正确性测试等。也可以叫做主题测试。
>
> -- Cosven

关键字一：垂直领域。数据库的特性何其多，纷繁杂乱，了解熟悉起来费时费脑，
需要很多很多工程师。工程师又叫什么？码农！我觉得这个称呼其实还是挺有有道理的。
学习数据库的小特性，就像了解一个高楼大厦的每块砖是如何码的一样，琐碎，费劲。
关键是，就算花了几辈子去学，可能还不如当初那些亲手码砖块的人熟悉那几块砖，
人家“码农”心里可能一清二楚。但如果你从万千砖块中提取一个**垂直领域**，
比如承重墙的码法，在这个领域上日积月累，那你可能就是“承重墙砖家”。【笑死】
分布式数据库领域可以有存储、计算、分布式砖家，也可以有性能、正确性、稳定性砖家嘛。
存储砖家加分项是有性能调优经验，那性能砖家加分项则可以是有存储设计经验，
稳定性砖家加分项可以是有分布式系统经验。

关键字二：大粒度。粒度大了，测试时就不需要去了解每个小特性的繁琐细节了。
把分布式数据库当成一个系统，把稳定性当成一个领域，针对这个领域进行的测试则是稳定性测试。
该测试的输入可以很简单，比如上面提到过的 SQL，以及**运维操作**。
以不同顺序在不同的系统**规格**下可以组合成为不同的**负载**。
大自然的混沌力量偶尔也会是成为一种输入，我愿称之为**错误注入**。
然后把类似“吞吐是否抖动”等作为测试的**检测点**，这测试就有了。
根本不需要关心数据库是如何解析、执行 SQL 的，也不需要关心数据是如何存储的。【爽歪歪】

### 系统测试自动化

凌晨了，有点困。`10-+1-6` 的一章。

- 负载=标准负载+随机负载+用户负载仿真。
- 运维=OPS As Code。
- 规格=软硬件配置。
- 错误注入=Chaos-Mesh。
- 监测点=领域知识+分布式原理+数据库原理+可测性。
- 高效=自动化。

有了自动化这个抓手，降本增效、赋能咱也行啊【狗头】TaaS 是个好东西【黄老爷笑脸】


## 待完善（TODO）

2023-05-25：这篇博客之前一个核心的思想是：开发应该对质量负责，
开发参与特性测试是一种高性价比的实践。测试则可以专注在测试效率提升，
以及系统测试的某一个垂直领域。但后来我在思考[职业规划](/blogs/91)的时候，
觉得这个思路还有一些可以补充的地方。主要是几个方面：开发应该怎样进行特性测试；
垂直领域之下的专业技能是什么；质量理念与测试活动。

### 测试分层
### 系统可测性

下面问题是否是可测性的需求的征召？
1. 对一个特性的质量没有把握时，没把握的原因会不会就是可测性不行？

对于可测性，自己感觉理解还比较浅，下面罗列一些常见的话题，以便以后学习
1. valuable（产生实际价值的）
   1. failpoint/syncpoint: 确定性的错误注入
   2. continous profiling: 性能问题定位
   3. invariants: 正确性测试
   4. metrics：问题定位，负载模式判定
2. unknown（可能有价值的）
   1. telemetry: 功能覆盖率？？
   2. 100% MC/DC test coverage (ref: sqlite3)：提升质量信心？
   3. valgrind：内存问题定位？
   4. tracing：问题定位？包括性能以及稳定性。

### 量化！大家都开心

## 带着故事睡觉

故事是讲不完的，睡觉它不香么！


[notes-about-qa]: https://github.com/cosven/database-testing#%E5%85%B3%E4%BA%8E-qa-%E8%BF%99%E4%B8%AA%E5%B2%97%E4%BD%8D
[max-blog-1]: https://cn.pingcap.com/blog/distributed-system-test-1
[gc-in-c-f]: https://docs.pingcap.com/zh/tidb/dev/garbage-collection-configuration#gc-in-compaction-filter-%E6%9C%BA%E5%88%B6
